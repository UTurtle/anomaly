{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가장 유사한 pair를 미리 저장합니다.\n",
    "\n",
    "- 유사도를 매번 계산하는것은 너무 큰 비용임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Load Dataset with Correct Paths\n",
    "# -----------------------------\n",
    "def load_dataset(attributes_file, class_dir, class_name):\n",
    "    if not os.path.isfile(attributes_file):\n",
    "        raise FileNotFoundError(f\"Attributes file not found: {attributes_file}\")\n",
    "\n",
    "    # CSV 파일에서 파일 이름과 라벨을 가져옴\n",
    "    df = pd.read_csv(attributes_file)\n",
    "    filenames = df['file_name'].tolist()\n",
    "    labels = ['anomaly' if 'anomaly' in name.lower() else 'normal' for name in filenames]\n",
    "    file_paths = [os.path.join(datasets_dir, f) for f in filenames]\n",
    "    print(file_paths)\n",
    "\n",
    "    return file_paths, labels\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Compute Spectrograms for All Files\n",
    "# -----------------------------\n",
    "def compute_all_spectrograms(file_paths, n_fft=160, hop_length=80, target_shape=(32, 32)):\n",
    "    spectrograms = {}\n",
    "    for path in tqdm(file_paths, desc=\"Computing Spectrograms\"):\n",
    "        # 경로 확인 및 정규화\n",
    "        norm_path = os.path.normpath(path)\n",
    "\n",
    "        if not os.path.exists(norm_path):\n",
    "            print(f\"File not found: {norm_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            y, sr = librosa.load(norm_path, sr=None)\n",
    "            S = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))\n",
    "            S_resized = librosa.util.fix_length(S, size=target_shape[0], axis=0)\n",
    "            S_resized = librosa.util.fix_length(S_resized, size=target_shape[1], axis=1)\n",
    "            spectrograms[norm_path] = S_resized\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {norm_path}: {e}. Skipping.\")\n",
    "\n",
    "    return spectrograms\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Find Best Matching Pair for Anomalies using SSIM\n",
    "# -----------------------------\n",
    "def find_best_matches_for_anomalies(anomaly_paths, normal_paths, spectrograms):\n",
    "    pairs = []\n",
    "    \n",
    "    for anomaly_path in tqdm(anomaly_paths, desc=\"Finding Best Matches\"):\n",
    "        # spectrograms 딕셔너리에 anomaly_path가 존재하는지 확인\n",
    "        anomaly_spectrogram = spectrograms.get(anomaly_path)\n",
    "        if anomaly_spectrogram is None:\n",
    "            continue\n",
    "\n",
    "        best_match = None\n",
    "        best_score = -1\n",
    "        \n",
    "        for normal_path in normal_paths:\n",
    "            normal_spectrogram = spectrograms.get(normal_path)\n",
    "            if normal_spectrogram is None:\n",
    "                continue\n",
    "\n",
    "            # data_range 파라미터를 추가하여 스펙트로그램의 최대값을 범위로 지정\n",
    "            score, _ = ssim(anomaly_spectrogram, normal_spectrogram, full=True, data_range=anomaly_spectrogram.max())\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = normal_path\n",
    "\n",
    "        pairs.append({\n",
    "            \"anomaly\": anomaly_path,\n",
    "            \"normal\": best_match,\n",
    "            \"similarity\": best_score,\n",
    "            \"method\": \"SSIM\"\n",
    "        })\n",
    "        \n",
    "    return pairs\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Save Pairs to CSV\n",
    "# -----------------------------\n",
    "def compute_and_save_all_class_pairs(datasets_dir, output_file=\"all_class_matching_pairs.csv\"):\n",
    "    all_pairs = []\n",
    "    \n",
    "    # 클래스별로 이상 및 정상 파일을 가져옴\n",
    "    for class_name in os.listdir(datasets_dir):\n",
    "        class_dir = os.path.join(datasets_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "\n",
    "        attributes_file = os.path.join(class_dir, \"attributes_00.csv\")\n",
    "        try:\n",
    "            # load_dataset 함수에 class_dir을 전달\n",
    "            file_paths, labels = load_dataset(attributes_file, class_dir, class_name)\n",
    "            anomaly_paths = [path for path, label in zip(file_paths, labels) if label == 'anomaly']\n",
    "            normal_paths = [path for path, label in zip(file_paths, labels) if label == 'normal']\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Attributes file not found for class: {class_name}\")\n",
    "            continue\n",
    "\n",
    "        # 스펙트로그램을 미리 계산하여 저장\n",
    "        all_paths = anomaly_paths + normal_paths\n",
    "        spectrograms = compute_all_spectrograms(all_paths)\n",
    "\n",
    "        # 각 이상 파일에 대해 가장 유사한 정상 파일을 찾아 페어링\n",
    "        pairs = find_best_matches_for_anomalies(anomaly_paths, normal_paths, spectrograms)\n",
    "        for pair in pairs:\n",
    "            pair[\"class\"] = class_name\n",
    "            all_pairs.append(pair)\n",
    "\n",
    "    # CSV로 저장\n",
    "    all_pairs_df = pd.DataFrame(all_pairs)\n",
    "    all_pairs_df.to_csv(output_file, index=False)\n",
    "    print(f\"All class matching pairs saved to {output_file}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load Matching Pairs from Saved CSV File\n",
    "# -----------------------------\n",
    "def load_all_class_pairs(output_file=\"all_class_matching_pairs.csv\"):\n",
    "    if os.path.exists(output_file):\n",
    "        all_pairs_df = pd.read_csv(output_file)\n",
    "        all_pairs = {}\n",
    "        \n",
    "        for _, row in all_pairs_df.iterrows():\n",
    "            class_name = row['class']\n",
    "            anomaly_path = row['anomaly']\n",
    "            normal_path = row['normal']\n",
    "            \n",
    "            if class_name not in all_pairs:\n",
    "                all_pairs[class_name] = []\n",
    "                \n",
    "            all_pairs[class_name].append({\n",
    "                \"anomaly\": anomaly_path,\n",
    "                \"normal\": normal_path,\n",
    "                \"method\": row[\"method\"],\n",
    "                \"similarity\": row[\"similarity\"]\n",
    "            })\n",
    "        \n",
    "        print(\"All class matching pairs loaded from CSV.\")\n",
    "        return all_pairs\n",
    "    else:\n",
    "        print(f\"No matching pairs file found at {output_file}. Please run compute_and_save_all_class_pairs first.\")\n",
    "        return {}\n",
    "\n",
    "# -----------------------------\n",
    "# Example Usage\n",
    "# -----------------------------\n",
    "datasets_dir = \"../../datasets/dev\"\n",
    "output_file = \"all_class_matching_pairs.csv\"\n",
    "\n",
    "compute_and_save_all_class_pairs(datasets_dir, output_file)\n",
    "all_class_pairs = load_all_class_pairs(output_file)\n",
    "\n",
    "class_name = \"gearbox\"  # 조회할 클래스명\n",
    "if class_name in all_class_pairs:\n",
    "    for pair in all_class_pairs[class_name]:\n",
    "        print(f\"Class: {class_name}, Anomaly: {pair['anomaly']}, Normal: {pair['normal']}, Method: {pair['method']}, Similarity: {pair['similarity']}\")\n",
    "else:\n",
    "    print(f\"No pairs found for class: {class_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomaly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
