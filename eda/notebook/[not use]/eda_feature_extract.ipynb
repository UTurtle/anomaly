{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note file 초기화\n",
    "\n",
    "- data pair마다 분석시 note를 추가하여 데이테 셋 마다 특징을 적어두는 게 좋을 것이라 판단.\n",
    "- 정확히 서로의 어디가 어떻게 다른지 분석하여 적어둘 필요 존재.\n",
    "- 얇고 넓게 알기 위해서 정말 쓸때 없어 보이는 데이터도 혹시 모르니 전부 생성해두는 것이 좋을 것이라 판단.\n",
    "\n",
    "\n",
    "- spectrogram만으로는 보기 어려울 수 있으니 여러가지 feature를 더 알아보도록 하자.\n",
    "    - class\n",
    "    - normal\n",
    "    - anomaly\n",
    "    - std\n",
    "    - avg\n",
    "    - ssim\n",
    "    - corss-crrelataion\n",
    "    - Spectral Centroid\n",
    "    - Spectral Bandwidth\n",
    "    - Spectral Flatness\n",
    "    - Spectral Roll-off\n",
    "    - RMS (Root Mean Square) 에너지\n",
    "    - Wavelet Transform\n",
    "    - power spectrum\n",
    "\n",
    "다음과 같이 열로 정리해서 넣는다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pywavelets\n",
      "  Downloading pywavelets-1.6.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.22.4 in /home/uturtle/anaconda3/envs/anomaly/lib/python3.9/site-packages (from pywavelets) (2.0.2)\n",
      "Downloading pywavelets-1.6.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pywavelets\n",
      "Successfully installed pywavelets-1.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pywavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: ToyCar\n",
      "['ToyCar/test/section_00_source_test_anomaly_0001_car_B1_spd_31V_mic_1.wav', 'ToyCar/test/section_00_source_test_anomaly_0002_car_B1_spd_34V_mic_1.wav', 'ToyCar/test/section_00_source_test_anomaly_0003_car_B1_spd_34V_mic_1.wav', 'ToyCar/test/section_00_source_test_anomaly_0004_car_B1_spd_40V_mic_1.wav', 'ToyCar/test/section_00_source_test_anomaly_0005_car_B1_spd_31V_mic_1.wav']\n",
      "../../../datasets/dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating features for ToyCar:   0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating features for ToyCar: 100%|██████████| 1200/1200 [05:10<00:00,  3.87it/s]\n",
      "Matching pairs in ToyCar: 100%|██████████| 100/100 [00:00<00:00, 59116.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: gearbox\n",
      "['gearbox/test/section_00_source_test_anomaly_0001_noAttribute.wav', 'gearbox/test/section_00_source_test_anomaly_0002_noAttribute.wav', 'gearbox/test/section_00_source_test_anomaly_0003_noAttribute.wav', 'gearbox/test/section_00_source_test_anomaly_0004_noAttribute.wav', 'gearbox/test/section_00_source_test_anomaly_0005_noAttribute.wav']\n",
      "../../../datasets/dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating features for gearbox: 100%|██████████| 1200/1200 [04:21<00:00,  4.59it/s]\n",
      "Matching pairs in gearbox: 100%|██████████| 100/100 [00:00<00:00, 68747.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: valve\n",
      "['valve/test/section_00_source_test_anomaly_0001_v1pat_none_v2pat_B.wav', 'valve/test/section_00_source_test_anomaly_0002_v1pat_none_v2pat_B.wav', 'valve/test/section_00_source_test_anomaly_0003_v1pat_none_v2pat_B.wav', 'valve/test/section_00_source_test_anomaly_0004_v1pat_none_v2pat_B.wav', 'valve/test/section_00_source_test_anomaly_0005_v1pat_none_v2pat_B.wav']\n",
      "../../../datasets/dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating features for valve: 100%|██████████| 1200/1200 [04:18<00:00,  4.64it/s]\n",
      "Matching pairs in valve: 100%|██████████| 100/100 [00:00<00:00, 64142.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: bearing\n",
      "['bearing/test/section_00_source_test_anomaly_0001_pro_A_vel_4_loc_A.wav', 'bearing/test/section_00_source_test_anomaly_0002_pro_A_vel_4_loc_A.wav', 'bearing/test/section_00_source_test_anomaly_0003_pro_A_vel_12_loc_A.wav', 'bearing/test/section_00_source_test_anomaly_0004_pro_A_vel_12_loc_A.wav', 'bearing/test/section_00_source_test_anomaly_0005_pro_A_vel_12_loc_A.wav']\n",
      "../../../datasets/dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating features for bearing: 100%|██████████| 1200/1200 [04:15<00:00,  4.70it/s]\n",
      "Matching pairs in bearing: 100%|██████████| 100/100 [00:00<00:00, 63559.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: slider\n",
      "['slider/test/section_00_source_test_anomaly_0001_noAttribute.wav', 'slider/test/section_00_source_test_anomaly_0002_noAttribute.wav', 'slider/test/section_00_source_test_anomaly_0003_noAttribute.wav', 'slider/test/section_00_source_test_anomaly_0004_noAttribute.wav', 'slider/test/section_00_source_test_anomaly_0005_noAttribute.wav']\n",
      "../../../datasets/dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating features for slider: 100%|██████████| 1200/1200 [04:17<00:00,  4.66it/s]\n",
      "Matching pairs in slider: 100%|██████████| 100/100 [00:00<00:00, 68523.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: fan\n",
      "['fan/test/section_00_source_test_anomaly_0001_n_A.wav', 'fan/test/section_00_source_test_anomaly_0002_n_A.wav', 'fan/test/section_00_source_test_anomaly_0003_n_A.wav', 'fan/test/section_00_source_test_anomaly_0004_n_A.wav', 'fan/test/section_00_source_test_anomaly_0005_n_A.wav']\n",
      "../../../datasets/dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating features for fan: 100%|██████████| 1200/1200 [04:17<00:00,  4.66it/s]\n",
      "Matching pairs in fan: 100%|██████████| 100/100 [00:00<00:00, 67770.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: ToyTrain\n",
      "['ToyTrain/test/section_00_source_test_anomaly_0001_noAttribute.wav', 'ToyTrain/test/section_00_source_test_anomaly_0002_noAttribute.wav', 'ToyTrain/test/section_00_source_test_anomaly_0003_noAttribute.wav', 'ToyTrain/test/section_00_source_test_anomaly_0004_noAttribute.wav', 'ToyTrain/test/section_00_source_test_anomaly_0005_noAttribute.wav']\n",
      "../../../datasets/dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating features for ToyTrain: 100%|██████████| 1200/1200 [05:12<00:00,  3.85it/s]\n",
      "Matching pairs in ToyTrain: 100%|██████████| 100/100 [00:00<00:00, 68300.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial CSV file saved to all_class_matching_pairs_with_features.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.signal import correlate\n",
    "from scipy.signal import cwt, ricker\n",
    "from scipy.fft import fft\n",
    "import pywt\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration Parameters\n",
    "# -----------------------------\n",
    "DATASETS_DIR = \"../../../datasets/dev\"  # 데이터셋 디렉토리 경로\n",
    "OUTPUT_CSV_FILE = \"all_class_matching_pairs_with_features.csv\"  # 결과를 저장할 CSV 파일 이름\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Load Dataset with Correct Paths\n",
    "# -----------------------------\n",
    "def load_dataset(attributes_file, datasets_dir):\n",
    "    if not os.path.isfile(attributes_file):\n",
    "        raise FileNotFoundError(f\"Attributes file not found: {attributes_file}\")\n",
    "\n",
    "    df = pd.read_csv(attributes_file)\n",
    "    filenames = df['file_name'].tolist()\n",
    "    labels = ['anomaly' if 'anomaly' in name.lower() else 'normal' for name in filenames]\n",
    "    \n",
    "    print(filenames[:5])\n",
    "    print(datasets_dir)\n",
    "    # 파일 경로를 생성할 때 datasets_dir, class_name, 파일명을 합침\n",
    "    file_paths = [os.path.join(datasets_dir, f) for f in filenames]\n",
    "\n",
    "    return file_paths, labels\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Find Matching Normal File Based on Filename Patterns\n",
    "# -----------------------------\n",
    "def find_matching_normal_file(anomaly_path, normal_paths):\n",
    "    anomaly_filename = os.path.basename(anomaly_path)\n",
    "    anomaly_parts = anomaly_filename.split(\"_\")\n",
    "\n",
    "    # 파일명 구조에 맞게 인덱스 조정\n",
    "    anomaly_number = anomaly_parts[5] if len(anomaly_parts) > 5 else None\n",
    "    anomaly_code = anomaly_parts[6] if len(anomaly_parts) > 6 else None\n",
    "\n",
    "    if anomaly_number is None:\n",
    "        print(f\"Unexpected file name format: {anomaly_filename}. No direct match found for anomaly file: {anomaly_path}\")\n",
    "        return None\n",
    "\n",
    "    for normal_path in normal_paths:\n",
    "        normal_filename = os.path.basename(normal_path)\n",
    "        normal_parts = normal_filename.split(\"_\")\n",
    "\n",
    "        normal_number = normal_parts[5] if len(normal_parts) > 5 else None\n",
    "        normal_code = normal_parts[6] if len(normal_parts) > 6 else None\n",
    "\n",
    "        if anomaly_code is None:\n",
    "            if anomaly_number == normal_number:\n",
    "                return normal_path\n",
    "        else:\n",
    "            if anomaly_number == normal_number and anomaly_code == normal_code:\n",
    "                return normal_path\n",
    "\n",
    "    # 매칭되는 정상 파일이 없는 경우\n",
    "    return None\n",
    "\n",
    "# -----------------------------\n",
    "# 추가: 특징 계산 함수\n",
    "# -----------------------------\n",
    "def calculate_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    features = {}\n",
    "    \n",
    "    # 기존 통계 및 스펙트럼 특징\n",
    "    features['std'] = np.std(y)\n",
    "    features['avg'] = np.mean(y)\n",
    "    features['rms'] = librosa.feature.rms(y=y).mean()\n",
    "    features['spectral_centroid'] = librosa.feature.spectral_centroid(y=y, sr=sr).mean()\n",
    "    features['spectral_bandwidth'] = librosa.feature.spectral_bandwidth(y=y, sr=sr).mean()\n",
    "    features['spectral_flatness'] = librosa.feature.spectral_flatness(y=y).mean()\n",
    "    features['spectral_rolloff'] = librosa.feature.spectral_rolloff(y=y, sr=sr).mean()\n",
    "    features['wavelet'] = calculate_wavelet_energy(y, wavelet='cmor1.5-1.0', scales=np.arange(1, 31))\n",
    "\n",
    "    \n",
    "    # Powerspectrum 계산\n",
    "    fft_result = fft(y)  # FFT 수행\n",
    "    power_spectrum = np.abs(fft_result)**2  # FFT 결과의 제곱\n",
    "    features['powerspectrum'] = power_spectrum.mean()  # 평균 파워 추가\n",
    "    \n",
    "    features['signal'] = y  # cross_correlation 계산을 위해 신호 저장\n",
    "    features['sr'] = sr  # 샘플링 레이트 저장\n",
    "    features['spectrogram'] = calculate_spectrogram(y, sr)  # ssim 계산을 위해 스펙트로그램 저장\n",
    "    return features\n",
    "\n",
    "\n",
    "def calculate_wavelet_energy(signal, wavelet='cmor1.5-1.0', scales=np.arange(1, 31)):\n",
    "    # CWT 계산 (PyWavelets)\n",
    "    coefficients, _ = pywt.cwt(signal, scales, wavelet)\n",
    "\n",
    "    # 에너지 계산 (계수의 제곱 합)\n",
    "    energy = np.sum(np.square(np.abs(coefficients)))\n",
    "    return energy\n",
    "\n",
    "\n",
    "# 추가: 스펙트로그램 계산 함수\n",
    "def calculate_spectrogram(y, sr, n_fft=512, hop_length=256):\n",
    "    S = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))\n",
    "    S_db = librosa.amplitude_to_db(S, ref=np.max)\n",
    "    return S_db\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Create Pairs and Save to CSV\n",
    "# -----------------------------\n",
    "def create_and_save_pairs_with_features(datasets_dir, output_file):\n",
    "    all_pairs = []\n",
    "\n",
    "    # 모든 스펙트로그램과 신호를 저장할 딕셔너리\n",
    "    all_features = {}\n",
    "\n",
    "    # 데이터셋 디렉토리 내의 각 클래스를s 순회\n",
    "    for class_name in os.listdir(datasets_dir):\n",
    "        class_dir = os.path.join(datasets_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing class: {class_name}\")\n",
    "\n",
    "        attributes_file = os.path.join(datasets_dir, class_name ,\"attributes_00.csv\")\n",
    "        try:\n",
    "            # 수정된 load_dataset 함수 호출\n",
    "            file_paths, labels = load_dataset(attributes_file, datasets_dir)\n",
    "            anomaly_paths = [path for path, label in zip(file_paths, labels) if label == 'anomaly']\n",
    "            normal_paths = [path for path, label in zip(file_paths, labels) if label == 'normal']\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Attributes file not found for class: {class_name}\")\n",
    "            continue\n",
    "\n",
    "        # 모든 파일의 특징을 미리 계산하여 저장\n",
    "        for path in tqdm(file_paths, desc=f\"Calculating features for {class_name}\"):\n",
    "            if not os.path.isfile(path):\n",
    "                print(f\"File not found: {path}. Skipping.\")\n",
    "                continue\n",
    "            try:\n",
    "                features = calculate_features(path)\n",
    "                all_features[path] = features\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {path}: {e}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "        # 각 이상치 파일에 대해 매칭되는 정상 파일을 찾음\n",
    "        for anomaly_path in tqdm(anomaly_paths, desc=f\"Matching pairs in {class_name}\"):\n",
    "            matching_normal_path = find_matching_normal_file(anomaly_path, normal_paths)\n",
    "\n",
    "            if matching_normal_path:\n",
    "                anomaly_features = all_features.get(anomaly_path)\n",
    "                normal_features = all_features.get(matching_normal_path)\n",
    "\n",
    "                if anomaly_features is None or normal_features is None:\n",
    "                    print(f\"Features missing for pair: {anomaly_path}, {matching_normal_path}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                pair_data = {\n",
    "                    \"class\": class_name,\n",
    "                    \"normal\": matching_normal_path,\n",
    "                    \"anomaly\": anomaly_path,\n",
    "                    \"handwrite\": \"\",  # handwrite 필드를 빈 문자열로 추가\n",
    "                    \"std\": anomaly_features['std'],\n",
    "                    \"avg\": anomaly_features['avg'],\n",
    "                    \"ssim\": \"\",  # ssim은 나중에 계산\n",
    "                    \"powerspectrum\": anomaly_features['powerspectrum'], \n",
    "                    \"cross_correlation\": \"\",  # cross_correlation도 나중에 계산\n",
    "                    \"Spectral Features\": f\"{anomaly_features['spectral_centroid']}, {anomaly_features['spectral_bandwidth']}, {anomaly_features['spectral_flatness']}, {anomaly_features['spectral_rolloff']}\",\n",
    "                    \"Spectral Centroid\": anomaly_features['spectral_centroid'],\n",
    "                    \"Spectral Bandwidth\": anomaly_features['spectral_bandwidth'],\n",
    "                    \"Spectral Flatness\": anomaly_features['spectral_flatness'],\n",
    "                    \"Spectral Roll-off\": anomaly_features['spectral_rolloff'],\n",
    "                    \"RMS (Root Mean Square)\": anomaly_features['rms'],\n",
    "                    \"Wavelet Transform\": anomaly_features['wavelet'],\n",
    "                    \"method\": \"Filename Matching\",\n",
    "                }\n",
    "\n",
    "                all_pairs.append(pair_data)\n",
    "            else:\n",
    "                print(f\"No matching normal file found for anomaly: {anomaly_path}\")\n",
    "\n",
    "    # DataFrame 생성\n",
    "    all_pairs_df = pd.DataFrame(all_pairs)\n",
    "    all_pairs_df.to_csv(output_file, index=False)\n",
    "    print(f\"Initial CSV file saved to {output_file}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 추가: ssim 및 cross_correlation 계산 함수\n",
    "# -----------------------------\n",
    "def calculate_ssim_and_cross_corr(all_pairs, output_file):\n",
    "    # ssim 및 cross_correlation 값을 저장할 리스트\n",
    "    ssim_list = []\n",
    "    cross_corr_list = []\n",
    "\n",
    "    for pair in tqdm(all_pairs, desc=\"Calculating ssim and cross_correlation\"):\n",
    "        # 스펙트로그램 가져오기\n",
    "        anomaly_spec = pair['anomaly_spectrogram']\n",
    "        normal_spec = pair['normal_spectrogram']\n",
    "\n",
    "        # ssim 계산\n",
    "        try:\n",
    "            ssim_value, _ = ssim(anomaly_spec, normal_spec, full=True, data_range=anomaly_spec.max() - anomaly_spec.min())\n",
    "        except ValueError:\n",
    "            ssim_value = np.nan  # 스펙트로그램 크기가 다를 경우 NaN 처리\n",
    "\n",
    "        # cross_correlation 계산\n",
    "        anomaly_signal = pair['anomaly_signal']\n",
    "        normal_signal = pair['normal_signal']\n",
    "\n",
    "        # 신호 길이를 맞추기 위해 짧은 쪽에 맞춤\n",
    "        min_length = min(len(anomaly_signal), len(normal_signal))\n",
    "        anomaly_signal = anomaly_signal[:min_length]\n",
    "        normal_signal = normal_signal[:min_length]\n",
    "\n",
    "        cross_corr = np.correlate(anomaly_signal, normal_signal, mode='valid')[0]\n",
    "\n",
    "        ssim_list.append(ssim_value)\n",
    "        cross_corr_list.append(cross_corr)\n",
    "\n",
    "    # DataFrame에 값 추가\n",
    "    df = pd.read_csv(output_file)\n",
    "    df['ssim'] = ssim_list\n",
    "    df['cross_correlation'] = cross_corr_list\n",
    "\n",
    "    # 최종 CSV 저장\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Final CSV file with ssim and cross_correlation saved to {output_file}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Main Execution\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    create_and_save_pairs_with_features(DATASETS_DIR, OUTPUT_CSV_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 한번에 csv에 저장하기에는 매우 큰 폴더임.\n",
    "\n",
    "- 클래스 별로 나누어서 저장한다면 훨씬 나을 것이라 판단. \n",
    "\n",
    "# WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.fft import fft\n",
    "import pywt\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings from librosa (optional)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration Parameters\n",
    "# -----------------------------\n",
    "DATASETS_DIR = \"../../../datasets/dev\"  # 데이터셋 디렉토리 경로\n",
    "FEATURES_DIR = \"features\"              # 특징 CSV 파일 저장 디렉토리\n",
    "HANDWRITE_DIR = \"handwrite\"            # Handwrite CSV 파일 저장 디렉토리\n",
    "\n",
    "# 폴더 생성\n",
    "os.makedirs(FEATURES_DIR, exist_ok=True)\n",
    "os.makedirs(HANDWRITE_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Load Dataset\n",
    "# -----------------------------\n",
    "def load_dataset(attributes_file, datasets_dir):\n",
    "    if not os.path.isfile(attributes_file):\n",
    "        raise FileNotFoundError(f\"Attributes file not found: {attributes_file}\")\n",
    "\n",
    "    df = pd.read_csv(attributes_file)\n",
    "    filenames = df['file_name'].tolist()\n",
    "    labels = ['anomaly' if 'anomaly' in name.lower() else 'normal' for name in filenames]\n",
    "    \n",
    "    # 파일 경로를 생성할 때 datasets_dir, 파일명을 합침\n",
    "    file_paths = [os.path.join(datasets_dir, f) for f in filenames]\n",
    "\n",
    "    return file_paths, labels\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Find Matching Normal File Based on Filename Patterns\n",
    "# -----------------------------\n",
    "def find_matching_normal_file(anomaly_path, normal_paths):\n",
    "    anomaly_filename = os.path.basename(anomaly_path)\n",
    "    anomaly_parts = anomaly_filename.split(\"_\")\n",
    "\n",
    "    # 파일명 구조에 맞게 인덱스 조정 (필요에 따라 변경)\n",
    "    # 예시: parts[5] = number, parts[6] = code\n",
    "    anomaly_number = anomaly_parts[5] if len(anomaly_parts) > 5 else None\n",
    "    anomaly_code = anomaly_parts[6] if len(anomaly_parts) > 6 else None\n",
    "\n",
    "    if anomaly_number is None:\n",
    "        print(f\"Unexpected file name format: {anomaly_filename}. No direct match found for anomaly file: {anomaly_path}\")\n",
    "        return None\n",
    "\n",
    "    for normal_path in normal_paths:\n",
    "        normal_filename = os.path.basename(normal_path)\n",
    "        normal_parts = normal_filename.split(\"_\")\n",
    "\n",
    "        normal_number = normal_parts[5] if len(normal_parts) > 5 else None\n",
    "        normal_code = normal_parts[6] if len(normal_parts) > 6 else None\n",
    "\n",
    "        if anomaly_code is None:\n",
    "            if anomaly_number == normal_number:\n",
    "                return normal_path\n",
    "        else:\n",
    "            if anomaly_number == normal_number and anomaly_code == normal_code:\n",
    "                return normal_path\n",
    "\n",
    "    # 매칭되는 정상 파일이 없는 경우\n",
    "    return None\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Calculate Features\n",
    "# -----------------------------\n",
    "def calculate_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    features = {}\n",
    "    \n",
    "    # 기존 통계 및 스펙트럼 특징\n",
    "    features['std'] = np.std(y)\n",
    "    features['avg'] = np.mean(y)\n",
    "    features['rms'] = librosa.feature.rms(y=y).mean()\n",
    "    features['spectral_centroid'] = librosa.feature.spectral_centroid(y=y, sr=sr).mean()\n",
    "    features['spectral_bandwidth'] = librosa.feature.spectral_bandwidth(y=y, sr=sr).mean()\n",
    "    features['spectral_flatness'] = librosa.feature.spectral_flatness(y=y).mean()\n",
    "    features['spectral_rolloff'] = librosa.feature.spectral_rolloff(y=y, sr=sr).mean()\n",
    "    features['wavelet'] = calculate_wavelet_energy(y, wavelet='cmor1.5-1.0', scales=np.arange(1, 31))\n",
    "\n",
    "    # Powerspectrum 계산\n",
    "    fft_result = fft(y)  # FFT 수행\n",
    "    power_spectrum = np.abs(fft_result)**2  # FFT 결과의 제곱\n",
    "    features['powerspectrum'] = power_spectrum.mean()  # 평균 파워 추가\n",
    "    \n",
    "    features['signal'] = y  # cross_correlation 계산을 위해 신호 저장\n",
    "    features['sr'] = sr      # 샘플링 레이트 저장\n",
    "    features['spectrogram'] = calculate_spectrogram(y, sr)  # ssim 계산을 위해 스펙트로그램 저장\n",
    "    return features\n",
    "\n",
    "def calculate_wavelet_energy(signal, wavelet='cmor1.5-1.0', scales=np.arange(1, 31)):\n",
    "    # CWT 계산 (PyWavelets)\n",
    "    coefficients, _ = pywt.cwt(signal, scales, wavelet)\n",
    "\n",
    "    # 에너지 계산 (계수의 제곱 합)\n",
    "    energy = np.sum(np.square(np.abs(coefficients)))\n",
    "    return energy\n",
    "\n",
    "def calculate_spectrogram(y, sr, n_fft=512, hop_length=256):\n",
    "    S = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))\n",
    "    S_db = librosa.amplitude_to_db(S, ref=np.max)\n",
    "    return S_db\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Create Pairs and Save to CSV Per Class\n",
    "# -----------------------------\n",
    "def create_and_save_pairs_with_features(datasets_dir, features_dir, handwrite_dir):\n",
    "    # 데이터셋 디렉토리 내의 각 클래스를 순회\n",
    "    for class_name in os.listdir(datasets_dir):\n",
    "        class_dir = os.path.join(datasets_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing class: {class_name}\")\n",
    "\n",
    "        attributes_file = os.path.join(class_dir, \"attributes_00.csv\")\n",
    "        try:\n",
    "            # load_dataset 함수 호출\n",
    "            file_paths, labels = load_dataset(attributes_file, datasets_dir)\n",
    "            anomaly_paths = [path for path, label in zip(file_paths, labels) if label == 'anomaly']\n",
    "            normal_paths = [path for path, label in zip(file_paths, labels) if label == 'normal']\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Attributes file not found for class: {class_name}\")\n",
    "            continue\n",
    "\n",
    "        # 모든 파일의 특징을 미리 계산하여 저장\n",
    "        all_features = {}\n",
    "        for path in tqdm(file_paths, desc=f\"Calculating features for {class_name}\"):\n",
    "            if not os.path.isfile(path):\n",
    "                print(f\"File not found: {path}. Skipping.\")\n",
    "                continue\n",
    "            try:\n",
    "                features = calculate_features(path)\n",
    "                all_features[path] = features\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {path}: {e}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "        # 각 이상치 파일에 대해 매칭되는 정상 파일을 찾음\n",
    "        features_output = os.path.join(features_dir, f\"features_{class_name}.csv\")\n",
    "        handwrite_output = os.path.join(handwrite_dir, f\"handwrite_{class_name}.csv\")\n",
    "\n",
    "        # CSV 파일이 이미 존재하면 삭제 (처음 실행 시)\n",
    "        if os.path.isfile(features_output):\n",
    "            os.remove(features_output)\n",
    "\n",
    "        # 컬럼 정의\n",
    "        columns = [\n",
    "            \"pair_id\", \"class\", \"normal\", \"anomaly\", \"handwrite\", \"std\", \"avg\", \"ssim\",\n",
    "            \"powerspectrum\", \"cross_correlation\", \"Spectral Features\",\n",
    "            \"Spectral Centroid\", \"Spectral Bandwidth\", \"Spectral Flatness\",\n",
    "            \"Spectral Roll-off\", \"RMS (Root Mean Square)\", \"Wavelet Transform\",\n",
    "            \"method\"\n",
    "        ]\n",
    "\n",
    "        # 빈 DataFrame을 생성하고 CSV로 저장 (헤더 포함)\n",
    "        pd.DataFrame(columns=columns).to_csv(features_output, index=False)\n",
    "\n",
    "        # Handwrite CSV 파일 초기화 (비어있는 상태)\n",
    "        if not os.path.isfile(handwrite_output):\n",
    "            pd.DataFrame(columns=[\"pair_id\", \"note\"]).to_csv(handwrite_output, index=False)\n",
    "\n",
    "        for anomaly_path in tqdm(anomaly_paths, desc=f\"Matching pairs in {class_name}\"):\n",
    "            matching_normal_path = find_matching_normal_file(anomaly_path, normal_paths)\n",
    "\n",
    "            if matching_normal_path:\n",
    "                anomaly_features = all_features.get(anomaly_path)\n",
    "                normal_features = all_features.get(matching_normal_path)\n",
    "\n",
    "                if anomaly_features is None or normal_features is None:\n",
    "                    print(f\"Features missing for pair: {anomaly_path}, {matching_normal_path}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # pair_id 생성 (예: anomaly 파일명 + normal 파일명)\n",
    "                pair_id = f\"{os.path.basename(anomaly_path)}_{os.path.basename(matching_normal_path)}\"\n",
    "\n",
    "                pair_data = {\n",
    "                    \"pair_id\": pair_id,\n",
    "                    \"class\": class_name,\n",
    "                    \"normal\": matching_normal_path,\n",
    "                    \"anomaly\": anomaly_path,\n",
    "                    \"handwrite\": \"\",  # handwrite 필드를 빈 문자열로 추가\n",
    "                    \"std\": anomaly_features['std'],\n",
    "                    \"avg\": anomaly_features['avg'],\n",
    "                    \"ssim\": \"\",  # ssim은 나중에 계산\n",
    "                    \"powerspectrum\": anomaly_features['powerspectrum'], \n",
    "                    \"cross_correlation\": \"\",  # cross_correlation도 나중에 계산\n",
    "                    \"Spectral Features\": f\"{anomaly_features['spectral_centroid']}, {anomaly_features['spectral_bandwidth']}, {anomaly_features['spectral_flatness']}, {anomaly_features['spectral_rolloff']}\",\n",
    "                    \"Spectral Centroid\": anomaly_features['spectral_centroid'],\n",
    "                    \"Spectral Bandwidth\": anomaly_features['spectral_bandwidth'],\n",
    "                    \"Spectral Flatness\": anomaly_features['spectral_flatness'],\n",
    "                    \"Spectral Roll-off\": anomaly_features['spectral_rolloff'],\n",
    "                    \"RMS (Root Mean Square)\": anomaly_features['rms'],\n",
    "                    \"Wavelet Transform\": anomaly_features['wavelet'],\n",
    "                    \"method\": \"Filename Matching\",\n",
    "                }\n",
    "\n",
    "                # DataFrame 생성 후 CSV에 추가 (append 모드)\n",
    "                pd.DataFrame([pair_data]).to_csv(features_output, mode='a', header=False, index=False)\n",
    "            else:\n",
    "                print(f\"No matching normal file found for anomaly: {anomaly_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Calculate SSIM and Cross Correlation\n",
    "# -----------------------------\n",
    "def calculate_ssim_and_cross_corr(features_dir):\n",
    "    for class_name in os.listdir(features_dir):\n",
    "        features_file = os.path.join(features_dir, f\"features_{class_name}.csv\")\n",
    "        if not os.path.isfile(features_file):\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(features_file)\n",
    "\n",
    "        ssim_list = []\n",
    "        cross_corr_list = []\n",
    "\n",
    "        for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Calculating ssim and cross_corr for {class_name}\"):\n",
    "            anomaly_path = row['anomaly']\n",
    "            normal_path = row['normal']\n",
    "\n",
    "            # 스펙트로그램 로드\n",
    "            try:\n",
    "                y_anomaly, sr_anomaly = librosa.load(anomaly_path, sr=None)\n",
    "                y_normal, sr_normal = librosa.load(normal_path, sr=None)\n",
    "\n",
    "                S_anomaly = calculate_spectrogram(y_anomaly, sr_anomaly)\n",
    "                S_normal = calculate_spectrogram(y_normal, sr_normal)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading spectrograms for pair: {anomaly_path}, {normal_path}. Setting ssim to NaN.\")\n",
    "                ssim_list.append(np.nan)\n",
    "                cross_corr_list.append(np.nan)\n",
    "                continue\n",
    "\n",
    "            # ssim 계산\n",
    "            try:\n",
    "                ssim_value = ssim(S_anomaly, S_normal, data_range=S_anomaly.max() - S_anomaly.min())\n",
    "            except ValueError:\n",
    "                ssim_value = np.nan  # 스펙트로그램 크기가 다를 경우 NaN 처리\n",
    "\n",
    "            ssim_list.append(ssim_value)\n",
    "\n",
    "            # cross_correlation 계산\n",
    "            try:\n",
    "                # 신호 로드\n",
    "                anomaly_signal, _ = librosa.load(anomaly_path, sr=None)\n",
    "                normal_signal, _ = librosa.load(normal_path, sr=None)\n",
    "\n",
    "                # 신호 길이를 맞추기 위해 짧은 쪽에 맞춤\n",
    "                min_length = min(len(anomaly_signal), len(normal_signal))\n",
    "                anomaly_signal = anomaly_signal[:min_length]\n",
    "                normal_signal = normal_signal[:min_length]\n",
    "\n",
    "                cross_corr = np.correlate(anomaly_signal, normal_signal, mode='valid')[0]\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating cross_correlation for pair: {anomaly_path}, {normal_path}. Setting cross_correlation to NaN.\")\n",
    "                cross_corr = np.nan\n",
    "\n",
    "            cross_corr_list.append(cross_corr)\n",
    "\n",
    "        # ssim 및 cross_corr 컬럼 업데이트\n",
    "        df['ssim'] = ssim_list\n",
    "        df['cross_correlation'] = cross_corr_list\n",
    "\n",
    "        # 수정된 DataFrame을 저장\n",
    "        df.to_csv(features_file, index=False)\n",
    "        print(f\"Updated {features_file} with ssim and cross_correlation\")\n",
    "\n",
    "# -----------------------------\n",
    "# Main Execution\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    create_and_save_pairs_with_features(DATASETS_DIR, FEATURES_DIR, HANDWRITE_DIR)\n",
    "    calculate_ssim_and_cross_corr(FEATURES_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomaly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
